Proposition détaillée
INF03 Identification des variantes d'un même fichier



Enjeux & motivations	2
État de l'art	3
Vue d'ensemble	3
Context-triggered piecewise hashing	4
SSDeep	4
SDHash	5
Locality-sensitive hashing	5
Nilsimsa	5
TLSH	6
Méthodes de travail	8
Timeline	8
Étude approfondie des solutions existantes	8
Recherche et développement	8
Bibliographie	9

Enjeux & motivations

Les motivations ayant conduit au choix de ce sujet sont nombreuses et témoignent de limportance de linformatique dans notre époque numérique. Il sagit déjà de participer à lavancement dun projet aux applications concrètes. La cybersécurité, en particulier, sest imposée comme un enjeu indispensable pour les particuliers comme les entreprises. Face à des menaces toujours plus sophistiquées, endiguer la propagation des logiciels malveillants savère de rigueur pour assurer le fonctionnement de nos infrastructures numériques. La détection de tels logiciels fait partie de cette lutte. Notre PSC se penche plus précisément sur la détection des variantes de fichiers malveillants connus.

Nous espérons pouvoir contribuer à cette lutte. Ensuite, nous provenons de filières différentes et n'avons pas tous eu la possibilité de travailler sur de longs projets informatiques auparavant, malgré notre intérêt pour ce domaine. Ce projet de 2e année représente donc une réelle opportunité pour nous. En effet, il nous offre la chance de nous familiariser avec la recherche, de comprendre et de développer des outils informatiques reposant sur de riches principes mathématiques, savoirs que nous serons probablement amenés à rencontrer de nouveau à lavenir.

Comme convenu avec nos tuteurs de la DGA, lenjeu de ce PSC est dans un premier lieu de réaliser un état de lart étendu des différentes méthodes permettant lidentification de fichiers similaires. Ce travail préliminaire vise à nous familiariser avec le domaine et nous permettre didentifier des pistes daméliorations de ces outils. Dans un second temps, il sera question de concevoir et dimplémenter des outils de détection de similitude entre deux fichiers.
État de l'art

La proposition détaillée doit présenter un recueil de l'état de l'art. Cependant, dans notre cas, une revue approfondie de l'état de l'art fait partie des attentes principales de nos tuteurs à la DGA. Elle devra donc être bien plus complète, ce sera lobjet dun travail sur plusieurs semaines qui culmine sur un compte rendu rendu à la DGA. En plus des algorithmes traditionnels (tlsh, ssdeep, ), il inclura peut-être des algorithmes de classifications utilisant des méthodes dapprentissage par machines et autres détournements de technologies existantes. La maturité sur le domaine acquise pendant cette période sera précieuse pour la suite de notre PSC.

Vue densemble
En informatique, le problème de savoir si un objet appartient à un ensemble est résolu par des techniques bien connues et très performantes. Pour savoir si un fichier appartient à une base de données, on aimerait donc sappuyer sur ces techniques. Pour ce faire, il faut réduire les fichiers - de lordre du kilo-octets - à des nombres de lordre de loctet.
Mathématiquement, on cherche à diminuer la dimension des fichiers à laide de fonctions de hash (on appelle h(f)  un digest ou signature):
h : F2d  F2d'  en supposant d d',
Considérons  F2d,  F2d'comme des espaces métriques munis de distances quelconques.  Classiquement, on exige de ces fonctions de hash la propriété suivante : pour f, f' deux fichiers, d(f, f') petit nimplique pas d'(h(f), h(f')) petit. Au contraire, on souhaite que la distance soit la plus grande possible. Utiliser ce type de fonctions pour savoir si un fichier appartient à une base de donnée permettrait dobtenir un résultat exact ; limitant les faux-positifs où le fichier est très similaire à lun de ceux présent dans la base. Les algorithmes les plus connus de hash satisfont cette propriété : md5, sha1, sha256, 

Dans notre cas, en revanche, nous souhaitons conserver la distance entre deux fichiers similaires :

d(f, f') petit d(h(f), h(f')) petit

Ce type de fonctions sont dites de fuzzy-hashing (hash flou). On aimerait notamment que ces fonctions conservent la distance dans le cas de translations, de permutations, et dinsertions.

Dans la suite, nous présentons deux types de techniques permettant de définir de telles fonctions.
Context-triggered piecewise hashing
SSDeep
Historiquement, cest le premier outil majeur de détection de logiciels exécutables similaires, qui sappuie sur Spamsum (2002), conçu pour détecter des courriers indésirables.

Lidée de lalgorithme est dabord de partitionner le fichier. Le choix des frontières est déterminé à laide de la méthode suivante. On définit une fenêtre glissante (context) de taille fixe, dont on calcule le hash. Si le hash atteint une trigger value, on marque loctet courant comme une frontière.  Une fois les frontières calculées, on calcule un hash pour chaque portion du fichier. La concaténation de tous ces hash forme le digest final.

Ce principe est appliqué deux fois avec des trigger values différentes : la signature finale de lentrée correspond à la concaténation des deux digests obtenus.  Finalement, le calcul de la distance dédition (au sens de Levenshtein) entre deux signatures permet destimer le niveau de similarité des fichiers.

Figure 1 - Pseudo-code de l'algorithme Spamsum (2002) qui utilise le CTPH pour détecter les nouveaux spams en les comparant à des spams connus

Pour généraliser, les algorithmes de CTPH peuvent être caractérisés par :
La fonction de hash utilisée sur la fenêtre glissante et la taille de celle-ci
La fonction de hash des portions
La fonction d'agrégation formant le digest
SDHash
Cet algorithme améliore deux aspects de SSDeep, le choix des frontières est déterminé en fonction de lentropie calculée pour la fenêtre glissante, et l'agrégation des hash de portions  se fait en utilisant des filtres de Bloom [5].  Ces améliorations diminuent drastiquement le nombre de faux-positifs.

Lidée dutiliser lentropie du fichier source est particulièrement intéressante : on arrive à utiliser la structure du fichier sans avoir aucune information préalable sur son type.


Locality-sensitive hashing
Nilsimsa
Nilsimsa est un exemple de locality-sensitive hashing [4]. À la différence des autres algorithmes présentés dans notre état de lart, Nilsimsa a été développé pour la détection de spam dans les mails. Il est intéressant de regarder son mode de fonctionnement, mais il est important de ne pas oublier que son objectif de départ est différent du nôtre.
La signature produite par Nilsimsa est une chaîne de 256 bits. On considère que deux fichiers sont semblables sils ont suffisamment de bits en commun.
Nilsimsa part dun tableau de 256 entiers initialisés à 0 appelé accumulateur. Lalgorithme extrait des trigrammes du texte. Chaque trigramme contribue à incrémenter lune des cases du tableau accumulateur à laide dune fonction de hachage, ce qui donne à la fin un histogramme des valeurs calculées. Chaque valeur de lhistogramme est finalement changée en un 1 ou un 0 suivant que lentier stocké dans la case est supérieur à un certain threshold (typiquement la valeur moyenne des cases sur un grand nombre de messages).
TLSH

Cet algorithme est le plus récent et présente les meilleures performances parmi ceux que nous présentons ici. Il reprend la génération de lhistogramme de Nilsimsa mais en améliorant son analyse.

Le digest produit est composé dun en-tête et dun corps. L'en-tête (headers) est formé de 6 caractères hexadécimaux, et caractérise globalement le fichier par sa longueur, une somme de contrôle ainsi que sur deux grandeurs liées à la dispersion des valeurs des compteurs associés aux buckets. Le corps (body) du digest décrit plus précisément comment les 128 buckets sont remplis. En fonction du taux de remplissage, une valeur de 0 à 3 lui est octroyée.

Si ssdeep et sdhash utilisent un score de 0 à 100 pour caractériser la similarité entre 2 fichiers, TLSH prône une approche différente, où lalgorithme caractérise à quel point deux fichiers sont différents par un score de distance entre ces fichiers, calculée selon un processus détaillé après. Une telle restriction des algorithmes concurrents limite le choix de seuils et conduit à des taux de faux positifs élevés ou à des taux de détection faibles [3].

Deux distances sont ajoutées, une entre les headers et une autre entre les bodies. Dans les deux cas, il sagit de calculer un équivalent dune distance de Hamming, mais avec un coût de pénalité plus important lors de situations caractérisées comme extrémales et donc peu probables.
En effet, si un bucket très rempli après traitement du 1er fichier ne lest quasiment pas après traitement du 2e, il est moins probable que les fichiers soient semblables.


Sur un échantillon de plus de 85OO fichiers similaires, la courbe de ROC (Receiver operating characteristic) évaluant la performance de cette méthode montre comment le taux de détection (true positive rate) varie en fonction du taux de faux positifs (false positive rate).



La qualité dune méthode est déterminée par laire sous la courbe, bien plus importante pour TLSH que pour ces concurrents, en grande partie permise par la flexibilité bien plus grande dans le choix du seuil. Cela permet un bon compromis entre détection et faux positifs.
Perspectives
Lun des critères intéressants pour évaluer les performances dun algorithme de fuzzy hashing est sa production de faux positifs et faux négatifs. Dans le cas de la détection de malware, on essaye de limiter les faux négatifs (qui correspondent à des fichiers malveillants non détectés). Pour la détection de spams, on essaye de limiter les faux positifs (qui correspondent aux messages légitimes qui ne seront pas lus). Déterminer pourquoi un programme produit beaucoup de faux positifs ou de faux négatifs est un problème difficile et intéressant.
Alors que Nilsimsa ne devait pas produire de faux positifs (ce qui est problématique dans le cas des spams), il a été montré quil en générait beaucoup plus que Ssdeep, Sdhash et TLSH (c.f Figure 3). Cela soulève le problème des tests suffisamment exhaustifs pour détecter ce type de comportement inattendu.
Nilsimsa est très résistant face aux attaques consistant à lisser lhistogramme par ajout de caractères aléatoire à la fin du fichier (le volume à ajouter est comparable à trois fois la taille du spam). Un tel comportement est courant dans la création de variantes de malware.
Un attaquant connaissant la fonction de hachage employée par Nilsimsa peut tenter de lisser lhistogramme en choisissant judicieusement les caractères quil ajoute en fin de message. Cet effet peut être contré par lutilisation de deux fonctions de hachage différentes, conduisant à deux digests complètement différents.
Figure 3 : tableau de comparaison de faux positifs et le taux de détection pour les algorithmes mentionnés dans létat de lart [3]

Méthodes de travail
Il a été convenu que les recherches comme l'exploration d'idées seront effectuées par binômes (tournants car nous sommes 5). Un mercredi après-midi sur deux sera consacré au compte-rendu du travail effectué par chaque binôme et à la répartition des tâches pour les binômes la semaine suivante. Par exemple, l'étude approfondie des algorithmes existants sera menée en étudiant successivement les algorithmes utilisés aujourd'hui pour en analyser les optimisations utilisées, les performances, les avantages et inconvénients, etc
Timeline
Le PSC se décompose en deux étapes majeures, qui peuvent être cloisonnées : exploration des techniques existantes, conception et développement de solutions innovantes.
Étude approfondie des solutions existantes
2 octobre : mieux comprendre les performances de TLSH, explorer lutilisation des spécificités des programmes exécutables, classifications
23 octobre : réunion de cadrage et début de rédaction
13 novembre: échéance du premier rendu de l'étude approfondie des programmes existants à la DGA. Celui-ci sera examiné par nos tuteurs qui nous demanderont éventuellement d'y apporter des retouches ou des ajouts.
27 novembre: échéance finale du rendu de l'étude approfondie.
Recherche et développement

4 décembre : formalisation des objectifs cibles, développement dune banque de tests pour évaluer les algorithmes conçus
18 décembre : début de la recherche et dimplémentations
15 janvier : rédaction du  rapport intermédiaire - pistes explorées et résultats,
élimination des solutions jugées impasses et définition de nouvelles pistes
24 janvier : rapport intermédiaire rendu
